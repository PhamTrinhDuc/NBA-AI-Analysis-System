## 1. Hi·ªÉu m·ª•c ti√™u d·ª± √°n v√† chu·∫©n b·ªã video
- D·ª± √°n n√†y nh·∫±m x√¢y d·ª±ng h·ªá th·ªëng ph√¢n t√≠ch b√≥ng r·ªï s·ª≠ d·ª•ng AI, k·∫øt h·ª£p c√°c m√¥ h√¨nh nh∆∞ YOLO v√† th∆∞ vi·ªán nh∆∞ OpenCV, Supervision.
- M·ª•c ti√™u l√† ph√¢n t√≠ch video tr·∫≠n ƒë·∫•u b√≥ng r·ªï ƒë·ªÉ r√∫t ra th√¥ng tin chuy√™n s√¢u nh∆∞:
  - C√°c ƒë∆∞·ªùng chuy·ªÅn b√≥ng
  - C√°c pha c·∫Øt b√≥ng
  - T·ª∑ l·ªá gi·ªØ b√≥ng
  - T·ªëc ƒë·ªô, kho·∫£ng c√°ch di chuy·ªÉn c·ªßa c·∫ßu th·ªß
  - Th·∫≠m ch√≠ chuy·ªÉn g√≥c quay sang b·∫£n ƒë·ªì chi·∫øn thu·∫≠t nh√¨n t·ª´ tr√™n cao.
- B·∫°n c·∫ßn t·∫£i v·ªÅ 3 video ƒë·∫ßu v√†o c·ª• th·ªÉ t·ª´ GitHub (ƒë√£ ƒë∆∞·ª£c ƒë√≠nh k√®m trong ph·∫ßn m√¥ t·∫£).
- C√°c video n√†y t∆∞∆°ng ·ª©ng v·ªõi 3 t√¨nh hu·ªëng:
  - Ch·∫°y v·ªõi b√≥ng ‚Üí t·ªët cho ph√¢n t√≠ch t·ªëc ƒë·ªô/kho·∫£ng c√°ch/gi·ªØ b√≥ng
  - Chuy·ªÅn b√≥ng ‚Üí ph·ª•c v·ª• logic chuy·ªÅn b√≥ng
  - C·∫Øt b√≥ng ‚Üí ph·ª•c v·ª• logic ngƒÉn ch·∫∑n
- T·∫°o m·ªôt th∆∞ m·ª•c m·ªõi cho d·ª± √°n, b√™n trong t·∫°o th∆∞ m·ª•c con t√™n input_videos ‚Üí ƒë·ªÉ ch·ª©a 3 video tr√™n.
- M·ªü th∆∞ m·ª•c d·ª± √°n b·∫±ng code editor (nh∆∞ Visual Studio Code).
## 2. C√†i ƒë·∫∑t m√¥ h√¨nh nh·∫≠n di·ªán ban ƒë·∫ßu v·ªõi YOLOv8
- B∆∞·ªõc c·ªët l√µi ƒë·∫ßu ti√™n l√† d√πng YOLO ƒë·ªÉ ph√°t hi·ªán c·∫ßu th·ªß v√† b√≥ng trong t·ª´ng khung h√¨nh c·ªßa video.
- YOLO l√† vi·∫øt t·∫Øt c·ªßa "You Only Look Once" ‚Äî m·ªôt m√¥ h√¨nh nh·∫≠n di·ªán ƒë·ªëi t∆∞·ª£ng m·∫°nh m·∫Ω, hi·ªán ƒë√£ ph√°t tri·ªÉn ƒë·∫øn phi√™n b·∫£n V11.
- Th∆∞ vi·ªán Ultralytics h·ªó tr·ª£ ch·∫°y c√°c phi√™n b·∫£n YOLO t·ª´ V1 ƒë·∫øn V11 r·∫•t ti·ªán l·ª£i.
C√†i ƒë·∫∑t Ultralytics b·∫±ng l·ªánh:
```bash
pip install ultralytics
```
- T·∫°o file main.py
- Import YOLO t·ª´ Ultralytics:
```bash
from ultralytics import YOLO
```
- T·∫£i m√¥ h√¨nh YOLOv8X ƒë√£ hu·∫•n luy·ªán s·∫µn:
```bash
model = YOLO("yolov8x.pt") (or yolov8x). 
```
  - C√°c phi√™n b·∫£n YOLOv8 g·ªìm: nano, small, medium, large, X-large
  - X-large (yolov8x) cho ƒë·ªô ch√≠nh x√°c cao nh·∫•t nh∆∞ng ƒë√≤i h·ªèi t√†i nguy√™n m·∫°nh h∆°n (RAM, GPU)
- Ch·∫°y m√¥ h√¨nh tr√™n video ƒë·∫ßu v√†o:
```bash
model.predict(source="input_videos/video_1.mp4", save=True)
```
  - L·ªánh tr√™n s·∫Ω x·ª≠ l√Ω t·ª´ng khung h√¨nh v√† l∆∞u l·∫°i video k·∫øt qu·∫£.
  - predict() tr·∫£ v·ªÅ k·∫øt qu·∫£ nh·∫≠n di·ªán: t·ªça ƒë·ªô bounding box, ƒë·ªô tin c·∫≠y (confidence), nh√£n l·ªõp (nh∆∞ "person", "sports ball").
  - In k·∫øt qu·∫£ khung h√¨nh ƒë·∫ßu ti√™n:
- In k·∫øt qu·∫£ khung h√¨nh ƒë·∫ßu ti√™n:
```bash
results = model.predict(...)
print(results[0].boxes)
```
- Video ƒë·∫ßu ra s·∫Ω n·∫±m trong th∆∞ m·ª•c: runs/detect/predict.
-  Ki·ªÉm tra video k·∫øt qu·∫£: th·∫•y m√¥ h√¨nh nh·∫≠n di·ªán ƒë∆∞·ª£c ng∆∞·ªùi (k·ªÉ c·∫£ kh√°n gi·∫£ - kh√¥ng mong mu·ªën) v√† b√≥ng c√≤n ch∆∞a ·ªïn ƒë·ªãnh.
## 3. Fine-tuning YOLO for Specific Object Detection (Players and Balls)
### M·ª•c ti√™u:
ƒê·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c v√† ƒë·∫£m b·∫£o m√¥ h√¨nh ch·ªâ nh·∫≠n di·ªán ƒë√∫ng c√°c ƒë·ªëi t∆∞·ª£ng nh∆∞ c·∫ßu th·ªß trong s√¢n v√† b√≥ng r·ªï, b·∫°n c·∫ßn fine-tune (hu·∫•n luy·ªán l·∫°i) m√¥ h√¨nh YOLO tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu ƒë∆∞·ª£c g√°n nh√£n t√πy ch·ªânh.
### Hi·ªÉu r√µ v·ªÅ Fine-tuning:
- Fine-tuning l√† m·ªôt d·∫°ng transfer learning ‚Äì b·∫°n t·∫≠n d·ª•ng m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc, r·ªìi "hu·∫•n luy·ªán l·∫°i nh·∫π" ƒë·ªÉ n√≥ ph√π h·ª£p h∆°n v·ªõi b√†i to√°n c·ª• th·ªÉ c·ªßa m√¨nh.
- C·∫ßn m·ªôt t·∫≠p d·ªØ li·ªáu c√≥ ·∫£nh v√† bounding box x√°c ƒë·ªãnh v·ªã tr√≠ c√°c ƒë·ªëi t∆∞·ª£ng b·∫°n mu·ªën nh·∫≠n di·ªán, v√≠ d·ª•: `player`, `referee`, `ball`, `hoop`
- Roboflow l√† n·ªÅn t·∫£ng ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t ƒë·ªÉ t√¨m ki·∫øm ho·∫∑c t·∫°o c√°c t·∫≠p d·ªØ li·ªáu c√≥ g√°n nh√£n.
### YOLOv5 ph√π h·ª£p h∆°n YOLOv8 cho sports analytics:
- M·∫∑c d√π `YOLOv8` hi·ªán ƒë·∫°i h∆°n, `YOLOv5` cho k·∫øt qu·∫£ ·ªïn ƒë·ªãnh h∆°n trong vi·ªác ph√°t hi·ªán c·∫ßu th·ªß v√† b√≥ng sau fine-tuning.
- Hu·∫•n luy·ªán m√¥ h√¨nh s√¢u y√™u c·∫ßu GPU ‚Üí D√πng Google Colab l√† gi·∫£i ph√°p t·ªëi ∆∞u (mi·ªÖn ph√≠, c√≥ GPU nh∆∞ T4).
### C√°c b∆∞·ªõc hu·∫•n luy·ªán tr√™n Google Colab:
#### 1. C√†i th∆∞ vi·ªán: 
```bash
!pip install roboflow ultralytics
```
#### 2. T·∫£i dataset t·ª´ Roboflow
```bash
from roboflow import Roboflow
rf = Roboflow(api_key="YOUR_API_KEY")
project = rf.workspace("your-workspace").project("basketball-players")
dataset = project.version(17).download("yolov5")
```
#### 3. ƒêi·ªÅu ch·ªânh c·∫•u tr√∫c th∆∞ m·ª•c (n·∫øu c·∫ßn):
```bash
import shutil
shutil.move('basketball-players-17', 'data')
```
#### 4. Hu·∫•n luy·ªán YOLOv5:
```bash
!yolo task=detect mode=train model=yolov5l6u.pt data=data/data.yaml epochs=100 imgsz=640 batch=8
# yolov5l6u.pt l√† m√¥ h√¨nh l·ªõn, t·ªëi ∆∞u cho ƒë·ªô ch√≠nh x√°c.
```
#### 5. T·∫£i file tr·ªçng s·ªë t·ªët nh·∫•t:
- T√¨m file:`runs/detect/train/weights/best.pt`
- T·∫£i v·ªÅ ‚Üí ƒê·∫∑t v√†o th∆∞ m·ª•c `models/` trong d·ª± √°n
- ƒê·ªïi t√™n theo m·ª•c ƒë√≠ch:
  - `player_detector.pt`
  - `ball_detector_model.pt`
- L·∫∑p l·∫°i c√°c b∆∞·ªõc ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh ri√™ng cho qu·∫£ b√≥ng:
  - S·ª≠ d·ª•ng c√πng t·∫≠p d·ªØ li·ªáu ho·∫∑c m·ªôt t·∫≠p ri√™ng v·ªÅ b√≥ng.
  - C√≥ th·ªÉ tƒÉng `epochs=250` ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t nh·∫≠n di·ªán b√≥ng.
### S·ª≠ d·ª•ng m√¥ h√¨nh fine-tuned trong code:
- Trong main.py, thay ƒëo·∫°n t·∫£i model YOLO nh∆∞ sau:
```bash
player_model = YOLO("models/player_detector.pt")
ball_model = YOLO("models/ball_detector_model.pt")
```
- Khi ƒë√£ c√≥ 2 m√¥ h√¨nh ri√™ng bi·ªát, b·∫°n c√≥ th·ªÉ √°p d·ª•ng c√°c k·ªπ thu·∫≠t nh∆∞:
  - l·ªçc v·ªã tr√≠ c·∫ßu th·ªß trong s√¢n
  - t√°ch ri√™ng b√≥ng kh·ªèi kh√°n gi·∫£
  - x√¢y b·∫£n ƒë·ªì chi·∫øn thu·∫≠t (top-down tactical map) ch√≠nh x√°c h∆°n
## 4. Set up Core Code Structure and Video Handling
### M·ª•c ti√™u:
- T·ªï ch·ª©c code g·ªçn g√†ng, d·ªÖ m·ªü r·ªông.
- Vi·∫øt c√°c h√†m ti·ªán √≠ch ƒë·ªÉ ƒë·ªçc v√† l∆∞u video b·∫±ng OpenCV.
### C·∫•u tr√∫c th∆∞ m·ª•c:
```bash
project_folder/
‚îÇ
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ input_videos/
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ video_utils.py
```
### 1. D·ªçn d·∫πp main.py
- X√≥a ƒëo·∫°n code demo ban ƒë·∫ßu (n·∫øu c√≥).
- Chuy·ªÉn sang s·ª≠ d·ª•ng main() chu·∫©n h√≥a.
### 2. T·∫°o th∆∞ m·ª•c ti·ªán √≠ch utils/
```bash
mkdir utils
touch utils/__init__.py
touch utils/video_utils.py
```
### 3. C√†i th∆∞ vi·ªán x·ª≠ l√Ω video:
```bash
pip install opencv-python
```
### 4. Vi·∫øt h√†m ƒë·ªçc v√† l∆∞u video trong `video_utils.py`
```bash
import cv2
import os

def read_video(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        frames.append(frame)
    cap.release()
    return frames

def save_video(output_frames, output_path, fps=30):
    if not output_frames:
        raise ValueError("No frames to save")

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    height, width, _ = output_frames[0].shape
    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # AVI format
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    for frame in output_frames:
        out.write(frame)

    out.release()
```
### 5. K·∫øt n·ªëi ti·ªán √≠ch trong `utils/__init__.py`
```bash
from .video_utils import read_video, save_video
```
### 6. Vi·∫øt l·∫°i `main.py` ƒë·ªÉ test
```bash
from utils import read_video, save_video

def main():
    input_path = "input_videos/video_1.mp4"
    output_path = "output_videos/test_output.avi"

    frames = read_video(input_path)
    print(f"S·ªë l∆∞·ª£ng frame: {len(frames)}")

    save_video(frames, output_path)
    print("Video ƒë√£ ƒë∆∞·ª£c l∆∞u.")

if __name__ == "__main__":
    main()
```
### K·∫øt qu·∫£ mong ƒë·ª£i:
- Video video_1.mp4 ƒë∆∞·ª£c ƒë·ªçc v√† l∆∞u l·∫°i th√†nh test_output.avi trong th∆∞ m·ª•c `output_videos/.`
- ƒêi·ªÅu n√†y ki·ªÉm tra r·∫±ng `read_video` v√† `save_video` ho·∫°t ƒë·ªông ·ªïn.

## 5. Tracking ƒë·ªëi t∆∞·ª£ng qua c√°c khung h√¨nh (Player Tracking)
### M·ª•c ti√™u:
- Sau khi YOLO detect ƒë∆∞·ª£c player, ta c·∫ßn track h·ªç xuy√™n su·ªët video.
- G√°n ID c·ªë ƒë·ªãnh cho t·ª´ng ng∆∞·ªùi ch∆°i ƒë·ªÉ t√≠nh t·ªëc ƒë·ªô, kho·∫£ng c√°ch, v.v.
### 1. T·∫°o module trackers/
```bash
mkdir trackers
touch trackers/__init__.py
touch trackers/player_tracker.py
```
### 2. C√†i th∆∞ vi·ªán Supervision:
```bash
pip install supervision
```
### 3. Vi·∫øt class `PlayerTracker` trong `player_tracker.py`
```bash
from ultralytics import YOLO
import supervision as sv

class PlayerTracker:
    def __init__(self, model_path="models/player_detector.pt"):
        # Load YOLO model ƒë√£ fine-tuned ƒë·ªÉ detect player
        self.model = YOLO(model_path)
        # Kh·ªüi t·∫°o tracker (ByteTrack)
        self.tracker = sv.ByteTrack()

    def detect_and_track_frames(self, frames, batch_size=20):
        tracked_frames = []
        # Chia frame th√†nh c√°c batch ƒë·ªÉ x·ª≠ l√Ω hi·ªáu qu·∫£ h∆°n
        for i in range(0, len(frames), batch_size):
            batch = frames[i:i+batch_size]
            # D·ª± ƒëo√°n b·∫±ng YOLO
            results = self.model.predict(batch, imgsz=640, verbose=False)
            for result in results:
                detections = sv.Detections.from_ultralytics(result)
                tracked = self.tracker.update_with_detections(detections)
                # L∆∞u th√¥ng tin: bounding box + ID track
                frame_data = []
                for box, track_id in zip(tracked.xyxy, tracked.tracker_id):
                    x1, y1, x2, y2 = box
                    frame_data.append({
                        "track_id": int(track_id),
                        "bbox": [int(x1), int(y1), int(x2), int(y2)]
                    })
                tracked_frames.append(frame_data)
        return tracked_frames
```
### 4. K·∫øt qu·∫£ ƒë·∫ßu ra:
- H√†m detect_and_track_frames() s·∫Ω tr·∫£ v·ªÅ m·ªôt list ch·ª©a th√¥ng tin tracking cho t·ª´ng frame:
```bash
[
  [  # Frame 1
    {"track_id": 1, "bbox": [100, 200, 180, 300]},
    {"track_id": 2, "bbox": [220, 210, 280, 320]},
  ],
  [  # Frame 2
    {"track_id": 1, "bbox": [102, 203, 182, 302]},
    {"track_id": 2, "bbox": [223, 213, 283, 323]},
  ],
  ...
]
```
## 6. Th√™m Stub Logic ƒë·ªÉ checkpoint k·∫øt qu·∫£ trung gian
### M·ª•c ti√™u:
- Video d√†i => m·ªói l·∫ßn ch·∫°y l·∫°i detect/track r·∫•t t·ªën th·ªùi gian.
- Ta s·∫Ω l∆∞u k·∫øt qu·∫£ ra file .pkl (pickle) sau khi x·ª≠ l√Ω.
- L·∫ßn ch·∫°y sau ch·ªâ c·∫ßn load l·∫°i, kh√¥ng c·∫ßn detect/track l·∫°i n·ªØa.
###  1. T·∫°o `stubs_utils.py` trong `utils`
```bash
touch utils/stubs_utils.py
```
### 2. C√†i ƒë·∫∑t pickle (n·∫øu c·∫ßn):
```bash
pip install pickle5
```
### 3. Code trong `utils/stubs_utils.py`
```bash
import os
import pickle

def save_stub(stub_path, obj):
    os.makedirs(os.path.dirname(stub_path), exist_ok=True)
    with open(stub_path, "wb") as f:
        pickle.dump(obj, f)

def read_stub(stub_path):
    if not os.path.exists(stub_path):
        return None
    with open(stub_path, "rb") as f:
        return pickle.load(f)
```
### 4. Trong utils/__init__.py, th√™m:
```bash
from .stubs_utils import save_stub, read_stub
```
### 5. T√≠ch h·ª£p stub v√†o PlayerTracker
Gi·∫£ s·ª≠ b·∫°n c√≥ ph∆∞∆°ng th·ª©c `detect_and_track_frames` nh∆∞ b∆∞·ªõc tr∆∞·ªõc, s·ª≠a l·∫°i nh∆∞ sau:
```bash
def detect_and_track_frames(self, frames, stub_path=None, read_from_stub=True):
    # N·∫øu c√≥ stub v√† cho ph√©p ƒë·ªçc
    if read_from_stub and stub_path:
        cached = read_stub(stub_path)
        if cached and len(cached) == len(frames):
            print(f"‚úî Loaded tracking data from {stub_path}")
            return cached
    # N·∫øu kh√¥ng c√≥ ho·∫∑c kh√¥ng h·ª£p l·ªá ‚Üí x·ª≠ l√Ω l·∫°i
    print("‚öô Detecting & tracking players...")
    tracked_frames = []
    for i in range(0, len(frames), 20):
        batch = frames[i:i+20]
        results = self.model.predict(batch, imgsz=640, verbose=False)
        for result in results:
            detections = sv.Detections.from_ultralytics(result)
            tracked = self.tracker.update_with_detections(detections)

            frame_data = []
            for box, track_id in zip(tracked.xyxy, tracked.tracker_id):
                x1, y1, x2, y2 = box
                frame_data.append({
                    "track_id": int(track_id),
                    "bbox": [int(x1), int(y1), int(x2), int(y2)]
                })
            tracked_frames.append(frame_data)
    # L∆∞u l·∫°i k·∫øt qu·∫£ sau x·ª≠ l√Ω
    if stub_path:
        save_stub(stub_path, tracked_frames)
        print(f"üíæ Saved tracking data to {stub_path}")
    return tracked_frames
```

### 6. C√°ch s·ª≠ d·ª•ng trong main.py
```bash
from trackers.player_tracker import PlayerTracker
from utils import read_video

def main():
    frames = read_video("input_videos/video_1.mp4")
    tracker = PlayerTracker()

    tracked_frames = tracker.detect_and_track_frames(
        frames,
        stub_path="stubs/video1_tracking.pkl",
        read_from_stub=True
    )
    # Ti·∫øp t·ª•c x·ª≠ l√Ω...
```
###  L·ª£i √≠ch:
- Ch·∫°y l·∫ßn ƒë·∫ßu: detect & track ‚Üí l∆∞u stub.
- Ch·∫°y l·∫ßn sau: load l·∫°i k·∫øt qu·∫£ ‚Üí nhanh g·∫•p 10x+.
## 7: Custom Annotation ‚Äì V·∫Ω bounding box
### M·ª•c ti√™u:
- Thay v√¨ c√°c bounding box m·∫∑c ƒë·ªãnh, ta s·∫Ω v·∫Ω ellipse b√™n d∆∞·ªõi ng∆∞·ªùi ch∆°i + ID.
- D·ªÖ xem h∆°n, gi·ªëng phong c√°ch c√°c h·ªá th·ªëng ph√¢n t√≠ch th·ªÉ thao th·ª±c t·∫ø.
- Create a drawers folder and an __init__.py file.
### 1. T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c:
```bash
mkdir drawers
touch drawers/__init__.py
touch drawers/player_tracks_drawer.py

mkdir utils
touch utils/bbox_utils.py  # n·∫øu ch∆∞a c√≥
```
### 2. PlayerTracksDrawer ‚Äì V·∫Ω d·ªØ li·ªáu track
```bash
from utils.bbox_utils import draw_ellipse

class PlayerTracksDrawer:
    def __init__(self):
        pass

    def draw(self, frames, player_tracks):
        annotated_frames = []

        for frame, tracks in zip(frames, player_tracks):
            frame_copy = frame.copy()

            for track in tracks:
                bbox = track["bbox"]
                track_id = track["track_id"]
                frame_copy = draw_ellipse(frame_copy, bbox, track_id)

            annotated_frames.append(frame_copy)

        return annotated_frames
```
### 3. draw_ellipse() ‚Äì V·∫Ω ellipse + ID
```bash
import cv2

def draw_ellipse(frame, bbox, track_id=None, color=(0, 255, 0)):
    x1, y1, x2, y2 = bbox
    center = (int((x1 + x2) / 2), int(y2))  # ƒë√°y c·ªßa bounding box
    axes = (int((x2 - x1) / 2), 10)  # chi·ªÅu ngang/vertical ellipse

    cv2.ellipse(frame, center, axes, 0, 0, 360, color, -1)

    if track_id is not None:
        text = f"ID: {track_id}"
        cv2.putText(frame, text, (x1, y1 - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    return frame
```
### 4. Th√™m import v√†o __init__.py:
- `drawers/__init__.py`: 
```bash
from .player_tracks_drawer import PlayerTracksDrawer
```
- `utils/__init__.py` (n·∫øu c·∫ßn):
```bash
from .bbox_utils import draw_ellipse
```
### 5. G·ªçi v·∫Ω trong main.py:
- Sau khi track xong:
```bash
from drawers import PlayerTracksDrawer

# Sau khi detect & track:
drawer = PlayerTracksDrawer()
annotated_frames = drawer.draw(frames, tracked_frames)

# L∆∞u video m·ªõi
save_video(annotated_frames, "outputs/video_1_annotated.avi")
```
### K·∫øt qu·∫£:
- M·ªói c·∫ßu th·ªß s·∫Ω c√≥ m·ªôt v√≤ng tr√≤n (ellipse) ph√≠a d∆∞·ªõi ng∆∞·ªùi ƒë·ªÉ ch·ªâ v·ªã tr√≠.
- C√≥ c·∫£ Track ID ƒë·ªÉ ph√¢n bi·ªát t·ª´ng ng∆∞·ªùi.
- TƒÉng t√≠nh chuy√™n nghi·ªáp v√† d·ªÖ quan s√°t cho video ph√¢n t√≠ch tr·∫≠n ƒë·∫•u.
## 8. Tinh ch·ªânh logic ph√°t hi·ªán b√≥ng
### M·ª•c ti√™u:
- Trong m·ªói frame, c√≥ th·ªÉ c√≥ nhi·ªÅu object ƒë∆∞·ª£c ph√°t hi·ªán l√† b√≥ng (ho·∫∑c sai l·ªách).
- C·∫ßn l·ªçc ra m·ªôt b√≥ng duy nh·∫•t c√≥ ƒë·ªô tin c·∫≠y cao nh·∫•t.
### 1. T·∫°o file ball_tracker.py
```bash
mkdir trackers  # n·∫øu ch∆∞a t·∫°o
touch trackers/ball_tracker.py
```
### 2. Logic ch·ªçn b√≥ng t·ªët nh·∫•t cho m·ªói frame:
```bash
from ultralytics import YOLO
import supervision as sv

class BallTracker:
    def __init__(self, model_path):
        self.model = YOLO(model_path)
        self.tracker = sv.ByteTrack()

    def detect_and_track_ball(self, frames):
        all_ball_detections = []

        results = self.model.predict(frames, stream=True, verbose=False)

        for frame_result in results:
            balls = []
            for box in frame_result.boxes:
                cls = int(box.cls[0])
                conf = float(box.conf[0])
                label = self.model.model.names[cls]

                if label == 'sports ball':  # ho·∫∑c 'ball' t√πy model
                    balls.append({
                        'bbox': box.xyxy[0].tolist(),  # [x1, y1, x2, y2]
                        'conf': conf
                    })

            # Ch·ªçn qu·∫£ b√≥ng c√≥ ƒë·ªô tin c·∫≠y cao nh·∫•t
            if balls:
                best_ball = max(balls, key=lambda x: x['conf'])
                all_ball_detections.append(best_ball)
            else:
                all_ball_detections.append(None)

        return all_ball_detections
```
### 3. C√°ch s·ª≠ d·ª•ng trong main.py
```bash
from trackers.ball_tracker import BallTracker

ball_tracker = BallTracker("models/ball_detector_model.pt")
ball_detections = ball_tracker.detect_and_track_ball(frames)
```
### 4. K·∫øt qu·∫£:
- M·ªói frame s·∫Ω tr·∫£ v·ªÅ:
- None n·∫øu kh√¥ng c√≥ b√≥ng.
- Ho·∫∑c {'bbox': [...], 'conf': ...} cho qu·∫£ b√≥ng c√≥ ƒë·ªô tin c·∫≠y cao nh·∫•t.
### G·ª£i √Ω m·ªü r·ªông:
- V·∫Ω b√≥ng b·∫±ng cv2.circle() thay v√¨ bbox ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng ƒë·∫πp.
- D√πng logic track_id n·∫øu mu·ªën theo d√µi qu·∫£ b√≥ng di chuy·ªÉn theo th·ªùi gian.
## 9. G√°n c·∫ßu th·ªß v√†o ƒë·ªôi b·∫±ng ph√¢n lo·∫°i kh√¥ng hu·∫•n luy·ªán (Zero-Shot Classification)
### M·ª•c ti√™u:
- Ph√¢n lo·∫°i c·∫ßu th·ªß v√†o 1 trong 2 ƒë·ªôi d·ª±a v√†o m√†u √°o (v√≠ d·ª•: "√°o tr·∫Øng", "√°o xanh ƒë·∫≠m") m√† kh√¥ng c·∫ßn hu·∫•n luy·ªán m√¥ h√¨nh m·ªõi.
### C√†i ƒë·∫∑t c·∫ßn thi·∫øt
```bash
pip install transformers torchvision pillow
```
### C·∫•u tr√∫c th∆∞ m·ª•c
```bash
team_assigner/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ team_assigner.py
```
### Logic g√°n ƒë·ªôi: team_assigner/team_assigner.py
```bash
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import torch

class TeamAssigner:
    def __init__(self, device='cpu'):
        self.device = device
        self.classes = ["white shirt", "dark blue shirt"]  # t√™n ƒë·ªôi d·ª±a v√†o m√†u √°o
        self.model = CLIPModel.from_pretrained("fashionclip/fashion-clip").to(device)
        self.processor = CLIPProcessor.from_pretrained("fashionclip/fashion-clip")

    def get_player_team(self, frame, bbox):
        x1, y1, x2, y2 = map(int, bbox)
        cropped = frame[y1:y2, x1:x2]  # crop ng∆∞·ªùi ch∆°i t·ª´ frame

        # Convert to PIL Image
        image = Image.fromarray(cropped)

        inputs = self.processor(
            text=self.classes,
            images=image,
            return_tensors="pt",
            padding=True
        ).to(self.device)

        outputs = self.model(**inputs)
        logits_per_image = outputs.logits_per_image  # scores
        probs = logits_per_image.softmax(dim=1)       # chuy·ªÉn th√†nh x√°c su·∫•t

        best_class = self.classes[probs.argmax().item()]
        return best_class  # tr·∫£ v·ªÅ t√™n ƒë·ªôi
```
### C√°ch s·ª≠ d·ª•ng trong main.py
```bash
from team_assigner.team_assigner import TeamAssigner

team_assigner = TeamAssigner(device='cuda' if torch.cuda.is_available() else 'cpu')

for frame_idx, frame in enumerate(frames):
    for player in player_tracks[frame_idx]:
        bbox = player['bbox']
        team = team_assigner.get_player_team(frame, bbox)
        player['team'] = team
```
### K·∫øt qu·∫£:
- M·ªói c·∫ßu th·ªß s·∫Ω ƒë∆∞·ª£c g√°n thu·ªôc v·ªÅ ‚Äúwhite shirt‚Äù ho·∫∑c ‚Äúdark blue shirt‚Äù.
- C√≥ th·ªÉ d√πng tr∆∞·ªùng player['team'] ƒë·ªÉ v·∫Ω m√†u vi·ªÅn kh√°c nhau ho·∫∑c th·ªëng k√™ theo ƒë·ªôi.
### M·ªü r·ªông (n·∫øu c·∫ßn):
- G√°n team_id (0, 1) thay v√¨ string.
- L∆∞u k·∫øt qu·∫£ n√†y b·∫±ng stub (pickle) ƒë·ªÉ kh√¥ng ph·∫£i ph√¢n lo·∫°i l·∫°i m·ªói l·∫ßn ch·∫°y.

## 10. X√°c ƒë·ªãnh ng∆∞·ªùi gi·ªØ b√≥ng (Ball Possession Detection)
### M·ª•c ti√™u:
Trong h·ªá th·ªëng ph√¢n t√≠ch tr·∫≠n ƒë·∫•u b√≥ng r·ªï, m·ªôt t√≠nh nƒÉng quan tr·ªçng l√† bi·∫øt ai ƒëang gi·ªØ b√≥ng t·∫°i t·ª´ng th·ªùi ƒëi·ªÉm. ƒêi·ªÅu n√†y l√† n·ªÅn t·∫£ng cho c√°c ph√¢n t√≠ch n√¢ng cao nh∆∞:
- T√≠nh % ki·ªÉm so√°t b√≥ng c·ªßa m·ªói ƒë·ªôi.
- Ph√°t hi·ªán c√°c ƒë∆∞·ªùng chuy·ªÅn, steal, rebound.
- T·∫°o video highlight t·ª± ƒë·ªông.
Do ƒë√≥, b∆∞·ªõc n√†y s·∫Ω so s√°nh v·ªã tr√≠ b√≥ng v·ªõi c√°c c·∫ßu th·ªß v√† x√°c ƒë·ªãnh ai ƒëang gi·ªØ b√≥ng ·ªü m·ªói frame.
### Logic ph√¢n t√≠ch "ball possession"
#### 1. L·∫•y v·ªã tr√≠ trung t√¢m b√≥ng trong m·ªói frame
- T·ª´ k·∫øt qu·∫£ c·ªßa YOLO ball detector (ball_positions), b·∫°n c√≥ to·∫° ƒë·ªô t√¢m c·ªßa b√≥ng trong m·ªói frame.
#### 2. L·∫•y c√°c bounding box c·ªßa c·∫ßu th·ªß t·ª´ player tracker
- M·ªói c·∫ßu th·ªß ƒë√£ ƒë∆∞·ª£c g√°n track ID c·ªë ƒë·ªãnh. M·ªói frame c√≥ danh s√°ch c·∫ßu th·ªß ƒëang xu·∫•t hi·ªán.
#### 3.T√≠nh kho·∫£ng c√°ch gi·ªØa b√≥ng v√† c√°c "ƒëi·ªÉm tr·ªçng y·∫øu" c·ªßa m·ªói c·∫ßu th·ªß
- Thay v√¨ ch·ªâ t√≠nh t·ª´ t√¢m c·∫ßu th·ªß ‚Üí b√≥ng, b·∫°n s·∫Ω l·∫•y nhi·ªÅu ƒëi·ªÉm ƒë·∫°i di·ªán nh∆∞: bottom center, center, left center, right center t·ª´ bounding box.
- T√≠nh kho·∫£ng c√°ch t·ª´ t√¢m b√≥ng ƒë·∫øn c√°c ƒëi·ªÉm n√†y.
#### 4. Ng∆∞·ªùi gi·ªØ b√≥ng = c·∫ßu th·ªß g·∫ßn b√≥ng nh·∫•t
- N·∫øu kho·∫£ng c√°ch < ng∆∞·ª°ng h·ª£p l√Ω (v√≠ d·ª• d∆∞·ªõi 50 pixels), ta xem c·∫ßu th·ªß ƒë√≥ ƒëang gi·ªØ b√≥ng.
- Tr√°nh nh·∫•p nh√°y (flickering) ‚Üí d√πng smoothing
#### 5. Ch·ªâ xem l√† "ƒëang gi·ªØ b√≥ng" n·∫øu c·∫ßu th·ªß ƒë√≥ g·∫ßn b√≥ng li√™n t·ª•c trong ‚â•11 frame.
- Tr√°nh t√¨nh tr·∫°ng b√≥ng b·ªã nh·∫ßm sang c·∫ßu th·ªß kh√°c ch·ªâ v√¨ m·ªôt frame l·ªách.
- L∆∞u k·∫øt qu·∫£ v√†o stub ƒë·ªÉ kh√¥ng ph·∫£i x·ª≠ l√Ω l·∫°i khi ch·∫°y l·∫°i ch∆∞∆°ng tr√¨nh.
### C·∫•u tr√∫c th∆∞ m·ª•c
```bash
ball_acquisition/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ ball_acquisition_detector.py

utils/
‚îú‚îÄ‚îÄ geometry_utils.py   # ch·ª©a c√°c h√†m ƒëo kho·∫£ng c√°ch
```
### C√†i th∆∞ vi·ªán c·∫ßn thi·∫øt
```bash
pip install numpy
```
### `utils/geometry_utils.py`
```bash
import numpy as np

def euclidean_distance(p1, p2):
    return np.linalg.norm(np.array(p1) - np.array(p2))

def get_key_points_from_bbox(bbox):
    x1, y1, x2, y2 = bbox
    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
    return [
        (cx, y2),         # bottom center
        (x1, cy),         # left center
        (x2, cy),         # right center
        (cx, cy)          # center
    ]
```
- V√¨ sao ch·ªçn nhi·ªÅu ƒëi·ªÉm ƒë·∫°i di·ªán?
    - Trong th·ª±c t·∫ø, b√≥ng kh√¥ng lu√¥n n·∫±m ·ªü gi·ªØa ng∆∞·ªùi ch∆°i.
    - ƒêi·ªÉm nh∆∞ ‚Äúbottom center‚Äù s·∫Ω g·∫ßn ch√¢n ‚Äì n∆°i b√≥ng th∆∞·ªùng hi·ªán di·ªán khi r√™ b√≥ng.
### `ball_acquisition/ball_acquisition_detector.py`
```bash
from utils.geometry_utils import euclidean_distance, get_key_points_from_bbox
from utils import save_stub, read_stub

class BallAcquisitionDetector:
    def __init__(self, min_hold_frames=11):
        self.min_hold_frames = min_hold_frames

    def detect_ball_possession(self, frames, player_tracks, ball_positions, read_from_stub=True, stub_path="stubs/ball_possession.pkl"):
        if read_from_stub:
            try:
                result = read_stub(stub_path)
                if result and len(result) == len(frames):
                    return result
            except:
                pass

        possession_history = []
        last_possessor = None
        consecutive_count = 0

        for frame_idx in range(len(frames)):
            ball = ball_positions[frame_idx]
            players = player_tracks[frame_idx]

            if not ball or not players:
                possession_history.append(None)
                continue

            bx, by = ball['center']
            min_distance = float("inf")
            possessor_id = None

            for player in players:
                pid = player['id']
                bbox = player['bbox']
                points = get_key_points_from_bbox(bbox)
                for pt in points:
                    dist = euclidean_distance(pt, (bx, by))
                    if dist < min_distance:
                        min_distance = dist
                        possessor_id = pid

            # smoothing possession
            if possessor_id == last_possessor:
                consecutive_count += 1
            else:
                consecutive_count = 1
                last_possessor = possessor_id

            if consecutive_count >= self.min_hold_frames:
                possession_history.append(possessor_id)
            else:
                possession_history.append(None)

        save_stub(stub_path, possession_history)
        return possession_history
```
### T√≠ch h·ª£p trong `main.py`
```bash
from ball_acquisition.ball_acquisition_detector import BallAcquisitionDetector

ball_acquisition = BallAcquisitionDetector()
ball_possessions = ball_acquisition.detect_ball_possession(
    frames,
    player_tracks=player_tracks,
    ball_positions=ball_detections
)
```
###  Output:
- ball_possessions: list ID c·∫ßu th·ªß gi·ªØ b√≥ng theo t·ª´ng frame.
- D√πng ƒë·ªÉ v·∫Ω hi·ªáu ·ª©ng, t·∫°o th·ªëng k√™ ho·∫∑c ph√°t hi·ªán c√°c s·ª± ki·ªán nh∆∞ pass, steal, shot.

## 11. T√≠nh to√°n quy·ªÅn ki·ªÉm so√°t b√≥ng theo ƒë·ªôi (Team Ball Control)
T·ª´ k·∫øt qu·∫£ b∆∞·ªõc 10 (ai gi·ªØ b√≥ng ·ªü m·ªói frame) v√† th√¥ng tin ƒë·ªôi (team assignment) c·ªßa m·ªói c·∫ßu th·ªß, ta s·∫Ω t√≠nh xem:
- ƒê·ªôi n√†o ƒëang ki·ªÉm so√°t b√≥ng ·ªü m·ªói frame.
- T·∫°o numpy array ch·ª©a team ID theo t·ª´ng frame: `1`, `2`, ho·∫∑c `-1` (kh√¥ng ai gi·ªØ b√≥ng).
- T·ª´ ƒë√≥ t√≠nh to√°n:
    - T·ªïng th·ªùi gian m·ªói ƒë·ªôi gi·ªØ b√≥ng.
    - % ki·ªÉm so√°t b√≥ng to√†n tr·∫≠n ho·∫∑c theo t·ª´ng ƒëo·∫°n video.
### Logic x·ª≠ l√Ω
#### Input:
- `ball_possessions`: list c√°c `player_id` ƒëang gi·ªØ b√≥ng t·∫°i m·ªói frame. (t·ª´ b∆∞·ªõc 10)
- `player_team_map`: dict d·∫°ng `{player_id: team_id}` do b∆∞·ªõc ph√¢n lo·∫°i m√†u √°o sinh ra (`team_assigner`).

#### Output:
- team_ball_control: numpy array ch·ª©a team ID ki·ªÉm so√°t b√≥ng t∆∞∆°ng ·ª©ng v·ªõi m·ªói frame:
    - `1` ‚Üí Team 1
    - `2` ‚Üí Team 2
    - `-1` ‚Üí Kh√¥ng r√µ (kh√¥ng ai gi·ªØ b√≥ng)
#### C·∫≠p nh·∫≠t class BallAcquisitionDetector
File: `ball_acquisition/ball_acquisition_detector.py`
- H√†m m·ªõi: `get_team_ball_control`
```bash
import numpy as np

class BallAcquisitionDetector:
    # ƒë√£ c√≥ __init__ v√† detect_ball_possession t·ª´ b∆∞·ªõc tr∆∞·ªõc

    def get_team_ball_control(self, ball_possessions, player_team_map, neutral_value=-1):
        """
        ball_possessions: list[player_id or None] (theo t·ª´ng frame)
        player_team_map: dict {player_id: team_id}
        return: numpy array [team_id] theo t·ª´ng frame
        """
        team_control = []

        for pid in ball_possessions:
            if pid is None or pid not in player_team_map:
                team_control.append(neutral_value)
            else:
                team_control.append(player_team_map[pid])

        return np.array(team_control)
```
### T√≠ch h·ª£p v√†o main.py
Sau khi ƒë√£ c√≥:
- ball_possessions t·ª´ b∆∞·ªõc 10.
- player_team_map sau b∆∞·ªõc team classification (v√≠ d·ª•: {3: 1, 7: 1, 12: 2, 15: 2}).
```bash
# V√≠ d·ª• team assignment: player_id ‚Üí team_id
player_team_map = {p['id']: p['team'] for frame in player_tracks for p in frame}

# Ch·∫°y b∆∞·ªõc 11:
team_control_array = ball_acquisition.get_team_ball_control(
    ball_possessions,
    player_team_map
)

# In th·ªëng k√™ % ki·ªÉm so√°t b√≥ng
total_frames = len(team_control_array)
team1_time = np.sum(team_control_array == 1)
team2_time = np.sum(team_control_array == 2)

print(f"Team 1 possession: {team1_time / total_frames * 100:.1f}%")
print(f"Team 2 possession: {team2_time / total_frames * 100:.1f}%")
```

## 12.V·∫Ω Overlay Th·ªëng K√™ L√™n Video
### M·ª•c ti√™u c·ªßa b∆∞·ªõc n√†y
Hi·ªÉn th·ªã th√¥ng tin ki·ªÉm so√°t b√≥ng c·ªßa ƒë·ªôi b√≥ng (Team Ball Control) trong t·ª´ng khung h√¨nh (frame), tr·ª±c ti·∫øp tr√™n video:
- Frame n√†o ƒëang ƒë∆∞·ª£c ki·ªÉm so√°t b·ªüi ƒë·ªôi n√†o? (VD: "Team 1 has the ball")
- N·∫øu kh√¥ng ai gi·ªØ b√≥ng th√¨ hi·ªán "Neutral".
### Logic x·ª≠ l√Ω
#### Input:
- `frame`: 1 frame trong video.
- `frame_number`: ch·ªâ s·ªë c·ªßa frame hi·ªán t·∫°i.
- `team_control_array`: numpy array ch·ª©a team ID ki·ªÉm so√°t b√≥ng ·ªü m·ªói frame (t·ª´ b∆∞·ªõc 11).
### Output:
 Frame ƒë√£ ƒë∆∞·ª£c v·∫Ω th√™m th√¥ng tin overlay:
- H·ªôp ch·ªØ ·ªü g√≥c tr√™n (ho·∫∑c d∆∞·ªõi)
- M√†u s·∫Øc ƒë·∫°i di·ªán t·ª´ng ƒë·ªôi
### C·∫≠p nh·∫≠t class BallAcquisitionDetector:
Trong `file ball_acquisition/ball_acquisition_detector.py`, th√™m:
```bash
import cv2

class BallAcquisitionDetector:
    # ... c√°c h√†m tr∆∞·ªõc
    def draw_frame(self, frame, frame_number, team_control_array):
        """
        V·∫Ω overlay ki·ªÉm so√°t b√≥ng l√™n frame
        """
        team_id = team_control_array[frame_number]
        if team_id == 1:
            color = (255, 0, 0)  # ƒê·ªôi 1: xanh d∆∞∆°ng
            text = "Team 1 has the ball"
        elif team_id == 2:
            color = (0, 0, 255)  # ƒê·ªôi 2: ƒë·ªè
            text = "Team 2 has the ball"
        else:
            color = (128, 128, 128)  # Kh√¥ng ai: x√°m
            text = "No team in control"
        # V·∫Ω background rectangle
        cv2.rectangle(frame, (10, 10), (310, 50), color, -1)
        # V·∫Ω text overlay
        cv2.putText(frame, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)

        return frame
```
### T√≠ch h·ª£p v√†o v√≤ng l·∫∑p ch√≠nh `main.py`:
```bash
ball_acquisition = BallAcquisitionDetector()

output_video_frames = []

for i, frame in enumerate(annotated_frames):
    # v·∫Ω overlay team control
    frame = ball_acquisition.draw_frame(frame, i, team_control_array)
    output_video_frames.append(frame)
```
## 13. X√°c ƒë·ªãnh ƒëi·ªÉm m·ªëc s√¢n ƒë·∫•u
### M·ª•c ti√™u
T·ª´ video th·∫≠t, x√°c ƒë·ªãnh c√°c keypoints s√¢n b√≥ng (v√≠ d·ª•: trung t√¢m s√¢n, 2 v·∫°ch 3 ƒëi·ªÉm, b·∫£ng r·ªï...) ‚Äì d√πng ƒë·ªÉ:
- L√†m homography (bi·∫øn ƒë·ªïi ph·ªëi c·∫£nh t·ª´ camera v·ªÅ g√≥c nh√¨n t·ª´ tr√™n xu·ªëng).
- √Åp v·ªã tr√≠ c·∫ßu th·ªß & b√≥ng v√†o s∆° ƒë·ªì chi·∫øn thu·∫≠t d·ªÖ ph√¢n t√≠ch.
### T∆∞ duy x·ª≠ l√Ω:
| Giai ƒëo·∫°n                                | M√¥ t·∫£                                      |
| ---------------------------------------- | ------------------------------------------ |
| 1. Fine-tune YOLOv8 pose model           | Hu·∫•n luy·ªán ƒë·ªÉ ph√°t hi·ªán keypoints s√¢n b√≥ng |
| 2. D·ª± ƒëo√°n keypoints m·ªói frame           | Tr√≠ch xu·∫•t v·ªã tr√≠ c√°c ƒëi·ªÉm landmark        |
| 3. T·ªëi ∆∞u h√≥a x·ª≠ l√Ω qua batch + l∆∞u stub | Tr√°nh l·∫∑p l·∫°i inference n·∫∑ng               |

### 1. Fine-tune YOLOv8 Pose Model (tr√™n Colab)
- D√πng model: yolov8x-pose.pt
- Dataset: t·ª´ Roboflow (g·∫Øn nh√£n c√°c ƒëi·ªÉm s√¢n)
- Command m·∫´u:
```bash
yolo task=pose mode=train model=yolov8x-pose.pt data=data.yaml epochs=500 imgsz=640 batch=16
```
### 2. C·∫•u tr√∫c th∆∞ m·ª•c
```bash
court_keypoint_detector/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ court_keypoint_detector.py
```
### 3. Code: CourtKeypointDetector
```bash
# court_keypoint_detector/court_keypoint_detector.py

from ultralytics import YOLO
import numpy as np
import os
from utils import read_stub, save_stub

class CourtKeypointDetector:
    def __init__(self, model_path="models/best.pt"):
        self.model = YOLO(model_path)

    def get_court_keypoints(self, frames, read_from_stub=True, stub_path="stubs/court_keypoints.pkl"):
        if read_from_stub and os.path.exists(stub_path):
            print("[Stub] Loading court keypoints from:", stub_path)
            return read_stub(stub_path)

        all_keypoints = []

        print("[INFO] Running court keypoint detection...")
        for i in range(0, len(frames), 20):  # x·ª≠ l√Ω theo batch
            batch = frames[i:i+20]
            results = self.model.predict(batch, stream=True)

            for result in results:
                keypoints = result.keypoints.xy.cpu().numpy()  # shape: [num_kpts, 2]
                all_keypoints.append(keypoints)

        save_stub(stub_path, all_keypoints)
        return all_keypoints
```
### 4. T√≠ch h·ª£p v√†o main.py
```bash
from court_keypoint_detector import CourtKeypointDetector

# Sau khi tracking c·∫ßu th·ªß xong:
court_detector = CourtKeypointDetector("models/best.pt")
court_keypoints = court_detector.get_court_keypoints(video_frames)

# Debug ki·ªÉm tra:
print("[DEBUG] Frame 0 Keypoints:", court_keypoints[0])
```
### Output: D·ªØ li·ªáu keypoints theo t·ª´ng frame
```bash
court_keypoints[0]  # -> array([[100.5, 222.1], [345.2, 120.4], ..., [x, y]])
```
- M·ªói frame c√≥ 5‚Äì10 ƒëi·ªÉm (t√πy s·ªë l∆∞·ª£ng keypoints b·∫°n d√°n nh√£n khi fine-tune).
- D·ªØ li·ªáu n√†y l√† ƒë·∫ßu v√†o c·ª±c k·ª≥ quan tr·ªçng ƒë·ªÉ:
  - Chuy·ªÉn g√≥c nh√¨n s√¢n b√≥ng v·ªÅ t·ª´ tr√™n xu·ªëng.
  - G·∫Øn t·ªça ƒë·ªô c·∫ßu th·ªß, b√≥ng v√†o tactical board.

## 14. Hi·ªÉn th·ªã tr·ª±c quan c√°c keypoint s√¢n b√≥ng
### M·ª•c ti√™u:
- V·∫Ω c√°c court keypoints (t·ªça ƒë·ªô landmark s√¢n b√≥ng) l√™n t·ª´ng frame c·ªßa video, bao g·ªìm:
- D·∫•u ch·∫•m t·∫°i v·ªã tr√≠ keypoint.
### Chi·∫øn l∆∞·ª£c:
- D√πng th∆∞ vi·ªán Supervision (sv), v·ªën t∆∞∆°ng th√≠ch t·ªët v·ªõi Ultralytics YOLO output.
- Hi·ªÉn th·ªã keypoints v·ªõi sv.vertex_annotator v√† sv.vertex_label_annotator.
- K·∫øt qu·∫£ l√† c√°c khung h√¨nh ƒë√£ annotated keypoints, c·ª±c h·ªØu √≠ch ƒë·ªÉ:
  - Debug ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu.
  - Chu·∫©n b·ªã mapping sang tactical board.
S·ªë th·ª© t·ª± keypoint (label) ƒë·ªÉ d·ªÖ debug v√† mapping
### 1. T·∫°o file `drawers/court_key_points_drawer.py`
```bash
# drawers/court_key_points_drawer.py

import numpy as np
import supervision as sv

class CourtKeyPointsDrawer:
    def __init__(self, keypoint_color=sv.Color.red()):
        self.vertex_annotator = sv.VertexAnnotator(color=keypoint_color, thickness=4)
        self.vertex_label_annotator = sv.VertexLabelAnnotator()

    def draw(self, frames, court_key_points):
        output_frames = []
        for i, frame in enumerate(frames):
            keypoints = court_key_points[i]

            if keypoints is None or len(keypoints) == 0:
                output_frames.append(frame.copy())
                continue
            # Convert to (N, 2) numpy array of int
            keypoints_arr = np.array(keypoints).astype(int)
            # Generate labels: ["0", "1", "2", ...]
            labels = [str(i) for i in range(len(keypoints_arr))]

            # Draw keypoints + labels using Supervision
            frame_copy = frame.copy()
            frame_copy = self.vertex_annotator.annotate(scene=frame_copy, vertices=keypoints_arr)
            frame_copy = self.vertex_label_annotator.annotate(scene=frame_copy, labels=labels, vertices=keypoints_arr)
            output_frames.append(frame_copy)
        return output_frames
```
### 2. Trong `drawers/__init__.py`
```bash
from .court_key_points_drawer import CourtKeyPointsDrawer
```
### 3. T√≠ch h·ª£p v√†o main.py
Sau khi b·∫°n ƒë√£ l·∫•y `court_keypoints` t·ª´ `CourtKeypointDetector`, th√™m:
```bash
from drawers import CourtKeyPointsDrawer
# Initialize Drawer
court_drawer = CourtKeyPointsDrawer()
# Annotate keypoints on video frames
video_frames_with_keypoints = court_drawer.draw(video_frames, court_keypoints)
# Update output frames
output_video_frames = video_frames_with_keypoints
```
### Output k·∫øt qu·∫£:
- Tr√™n m·ªói khung h√¨nh, c√°c keypoints s·∫Ω ƒë∆∞·ª£c v·∫Ω nh∆∞ ch·∫•m ƒë·ªè v·ªõi nh√£n ƒë√°nh s·ªë.
- H·ªØu √≠ch ƒë·ªÉ:
  - So s√°nh c√°c keypoints gi·ªØa c√°c frame ‚Üí ki·ªÉm tra ƒë·ªô ·ªïn ƒë·ªãnh.
  - T·ª± ƒë·ªông h√≥a matching v·ªõi s∆° ƒë·ªì s√¢n m·∫´u trong b∆∞·ªõc homography s·∫Øp t·ªõi.
## 15: T·∫°o Tactical View b·∫±ng Perspective Transformation
### M·ª•c ti√™u:
- Chuy·ªÉn ƒë·ªïi v·ªã tr√≠ pixel trong camera view th√†nh t·ªça ƒë·ªô th·ª±c t·∫ø/met ho·∫∑c tactical pixel map.
- D√πng homography matrix t·ª´ OpenCV ƒë·ªÉ chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô t·ª´ keypoint tr√™n s√¢n sang b·∫£n ƒë·ªì chi·∫øn thu·∫≠t (·∫£nh s∆° ƒë·ªì s√¢n).

### 1. C·∫•u tr√∫c th∆∞ m·ª•c
```bash
tactical_view_converter/
‚îú‚îÄ‚îÄ __init__.py
‚îî‚îÄ‚îÄ tactical_view_converter.py
```
### 2. `tactical_view_converter.py` ‚Äì Chuy·ªÉn ƒë·ªïi to·∫° ƒë·ªô
```bash
# tactical_view_converter.py

import cv2
import numpy as np

class TacticalViewConverter:
    def __init__(self, tactical_court_image_path, court_width_m, court_height_m, tactical_width_px, tactical_height_px):
        self.tactical_court_image_path = tactical_court_image_path
        self.court_width_m = court_width_m
        self.court_height_m = court_height_m
        self.tactical_width_px = tactical_width_px
        self.tactical_height_px = tactical_height_px
        self.homography_matrix = None

    def generate_perspective_transform_matrix(self, camera_keypoints, tactical_keypoints):
        # Convert to np.float32 arrays
        cam_pts = np.array(camera_keypoints, dtype=np.float32)
        tac_pts = np.array(tactical_keypoints, dtype=np.float32)
        self.homography_matrix, _ = cv2.findHomography(cam_pts, tac_pts)

    def convert_pixel_to_tactical(self, points):
        # points: (N, 2)
        points = np.array(points, dtype=np.float32).reshape(-1, 1, 2)
        transformed = cv2.perspectiveTransform(points, self.homography_matrix)
        return transformed.reshape(-1, 2)

    def get_player_positions(self, all_player_boxes, court_keypoints):
        # all_player_boxes: List[List[ [x1, y1, x2, y2] ]]
        all_positions = []

        for frame_boxes in all_player_boxes:
            frame_positions = []
            for box in frame_boxes:
                # Use bottom-center point of bbox
                x_center = (box[0] + box[2]) / 2
                y_bottom = box[3]
                tactical_pos = self.convert_pixel_to_tactical([(x_center, y_bottom)])
                frame_positions.append(tactical_pos[0])
            all_positions.append(frame_positions)

        return all_positions
```
### 3. `tactical_view_drawer.py` ‚Äì V·∫Ω tactical overlay
```bash
# drawers/tactical_view_drawer.py

import cv2
import numpy as np

class TacticalViewDrawer:
    def __init__(self, overlay_opacity=0.6, point_color=(0, 0, 255)):
        self.overlay_opacity = overlay_opacity
        self.point_color = point_color

    def draw(self, frames, court_img_path, width_px, height_px, tactical_keypoints):
        court_img = cv2.imread(court_img_path)
        court_img = cv2.resize(court_img, (width_px, height_px))

        output_frames = []
        for frame in frames:
            overlay = frame.copy()
            overlay[0:height_px, 0:width_px] = court_img
            # Draw keypoints on court image
            for i, pt in enumerate(tactical_keypoints):
                pt_int = tuple(int(x) for x in pt)
                cv2.circle(overlay, pt_int, 5, self.point_color, -1)
                cv2.putText(overlay, str(i), pt_int, cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.point_color, 1)

            blended = cv2.addWeighted(overlay, self.overlay_opacity, frame, 1 - self.overlay_opacity, 0)
            output_frames.append(blended)
        return output_frames
```
### 4. drawers/init.py
```bash
from .tactical_view_drawer import TacticalViewDrawer
```
### 5. T√≠ch h·ª£p v√†o `main.py`
```bash
from tactical_view_converter.tactical_view_converter import TacticalViewConverter
from drawers import TacticalViewDrawer
# Step 1: Init converter
converter = TacticalViewConverter(
    tactical_court_image_path='assets/tactical_court.png',
    court_width_m=28,  # standard basketball court
    court_height_m=15,
    tactical_width_px=560,
    tactical_height_px=300
)
# Step 2: T·∫°o ma tr·∫≠n chuy·ªÉn ƒë·ªïi t·ª´ keypoints
converter.generate_perspective_transform_matrix(camera_keypoints, tactical_keypoints)
# Step 3: Chuy·ªÉn to·∫° ƒë·ªô c·∫ßu th·ªß sang tactical map
tactical_player_positions = converter.get_player_positions(all_player_boxes, camera_keypoints)
# Step 4: Hi·ªÉn th·ªã tactical court overlay l√™n video
drawer = TacticalViewDrawer()
output_frames = drawer.draw(video_frames, 'assets/tactical_court.png', 560, 300, tactical_keypoints)
```
### K·∫øt qu·∫£:
- T·∫°o b·∫£n ƒë·ªì chi·∫øn thu·∫≠t b√™n g√≥c video, v·∫Ω overlay ·∫£nh s√¢n b√≥ng + c√°c keypoints x√°c ƒë·ªãnh ƒë√∫ng g√≥c nh√¨n.
- Gi√∫p d·ªÖ d√†ng v·∫Ω heatmap, ƒë∆∞·ªùng ch·∫°y, t·∫•n c√¥ng/ph√≤ng th·ªß sau n√†y.
### L∆∞u √Ω quan tr·ªçng:
| Y·∫øu t·ªë                 | Gi·∫£i th√≠ch                                                                  |
| ---------------------- | --------------------------------------------------------------------------- |
| `camera_keypoints`     | To·∫° ƒë·ªô landmark s√¢n (trong frame) ‚Üí n√™n d√πng c√°c ƒëi·ªÉm c·ªë ƒë·ªãnh nh∆∞ 4 g√≥c s√¢n |
| `tactical_keypoints`   | T·ªça ƒë·ªô t∆∞∆°ng ·ª©ng trong tactical map (c·ªë ƒë·ªãnh)                               |
| `get_player_positions` | Tr·∫£ v·ªÅ list c√°c to·∫° ƒë·ªô c·∫ßu th·ªß tr√™n tactical map ƒë·ªÉ sau n√†y v·∫Ω chi·∫øn thu·∫≠t  |

## 16: Validate v√† L·ªçc Court Keypoints
### M·ª•c ti√™u:
- L·ªçc b·ªè c√°c keypoint sai l·ªách g√¢y l·ªói khi t√≠nh ma tr·∫≠n homography.
- D·ª±a tr√™n nguy√™n l√Ω t·ª∑ l·ªá kho·∫£ng c√°ch gi·ªØa c√°c c·∫∑p ƒëi·ªÉm c·ªë ƒë·ªãnh (v√≠ d·ª•: ƒëi·ªÉm 13 ‚Üî 14 lu√¥n c√°ch nhau 5m th·ª±c t·∫ø). 
### 1. Th√™m v√†o TacticalViewConverter class:
```bash
import numpy as np
import copy

class TacticalViewConverter:
    # ... c√°c h√†m tr∆∞·ªõc ...

    def validate_key_points(self, court_keypoints, reference_indices=(13, 14), expected_real_distance=5.0, min_ratio=0.5, max_ratio=0.8):
        """
        court_keypoints: List of [x, y] positions
        reference_indices: tuple of two indices to use as reference for proportionality
        expected_real_distance: known real-world distance (meters or pixels) between reference points
        """
        valid_keypoints = copy.deepcopy(court_keypoints)

        idx_a, idx_b = reference_indices
        ref_a = np.array(court_keypoints[idx_a])
        ref_b = np.array(court_keypoints[idx_b])

        if np.all(ref_a == 0) or np.all(ref_b == 0):
            print("[Warning] Reference keypoints are invalid ‚Äî skipping validation.")
            return valid_keypoints

        reference_pixel_distance = np.linalg.norm(ref_a - ref_b)
        if reference_pixel_distance == 0:
            print("[Warning] Reference distance is zero ‚Äî invalid keypoints.")
            return valid_keypoints

        # T√≠nh t·ªâ l·ªá "1 ƒë∆°n v·ªã th·ª±c t·∫ø ‚âà bao nhi√™u pixel"
        pixel_per_unit = reference_pixel_distance / expected_real_distance

        for i, point in enumerate(court_keypoints):
            if i in reference_indices or np.all(np.array(point) == 0):
                continue

            pt = np.array(point)
            d1 = np.linalg.norm(pt - ref_a)
            d2 = np.linalg.norm(pt - ref_b)
            avg_distance = (d1 + d2) / 2

            ratio = avg_distance / reference_pixel_distance

            if ratio < min_ratio or ratio > max_ratio:
                valid_keypoints[i] = [0, 0]  # lo·∫°i b·ªè
                print(f"[Validation] Keypoint {i} invalid (ratio={ratio:.2f}), set to [0, 0]")

        return valid_keypoints
```

- Gi·∫£i th√≠ch: 

| Th√†nh ph·∫ßn                   | M·ª•c ƒë√≠ch                                                 |
| ---------------------------- | -------------------------------------------------------- |
| `reference_indices=(13, 14)` | C·∫∑p keypoint c·ªë ƒë·ªãnh tr√™n s√¢n d√πng ƒë·ªÉ l√†m chu·∫©n          |
| `expected_real_distance=5.0` | Kho·∫£ng c√°ch th·ª±c t·∫ø (5m) gi·ªØa 2 ƒëi·ªÉm                     |
| `ratio < 0.5 or > 0.8`       | N·∫øu ƒëi·ªÉm qu√° g·∫ßn/qu√° xa so v·ªõi chu·∫©n ‚Üí sai l·ªách, lo·∫°i b·ªè |
| `copy.deepcopy()`            | Tr√°nh thay ƒë·ªïi d·ªØ li·ªáu g·ªëc g√¢y side effect               |
### 2. T√≠ch h·ª£p v√†o main.py:
```bash
# Sau khi l·∫•y court_keypoints
raw_keypoints = court_keypoint_detector.get_court_keypoints(frames, read_from_stub=True)

# Validate
valid_keypoints = []
for frame_kp in raw_keypoints:
    validated = converter.validate_key_points(
        court_keypoints=frame_kp,
        reference_indices=(13, 14),
        expected_real_distance=5.0  # v√≠ d·ª• 5m th·∫≠t
    )
    valid_keypoints.append(validated)

# D√πng valid_keypoints ƒë·ªÉ t√≠nh ma tr·∫≠n
converter.generate_perspective_transform_matrix(camera_keypoints=valid_keypoints[0], tactical_keypoints=known_tactical_positions)
```
### K·∫øt qu·∫£:
- C√°c keypoint b·ªã l·ªách ho·∫∑c sai s·ªë l·ªõn s·∫Ω b·ªã lo·∫°i kh·ªèi ph√©p bi·∫øn ƒë·ªïi.
- B·∫£o v·ªá h·ªá th·ªëng kh·ªèi l·ªói "gi·∫≠t lag", bi·∫øn d·∫°ng b·∫£n ƒë·ªì chi·∫øn thu·∫≠t ho·∫∑c sai v·ªã tr√≠ c·∫ßu th·ªß.
## 17 Transform Player Positions to Tactical View
### M·ª•c ti√™u:
- D√πng c√°c court keypoints ƒë√£ ƒë∆∞·ª£c validate ƒë·ªÉ t·∫°o ma tr·∫≠n bi·∫øn ƒë·ªïi ph·ªëi c·∫£nh.
- D√πng ma tr·∫≠n n√†y ƒë·ªÉ bi·∫øn ƒë·ªïi c√°c v·ªã tr√≠ c·∫ßu th·ªß trong m·ªói frame ‚Üí sang tactical map (top-down view).
- V·∫Ω l·∫°i v·ªã tr√≠ c√°c c·∫ßu th·ªß l√™n b·∫£n ƒë·ªì s√¢n (tactical view).
### 1. Th√™m v√†o TacticalViewConverter class:
```bash
import cv2
import numpy as np

class TacticalViewConverter:
    def __init__(self, tactical_image_shape=(940, 500), real_world_size_m=(28.0, 15.0)):
        self.tactical_width_px, self.tactical_height_px = tactical_image_shape
        self.real_width_m, self.real_height_m = real_world_size_m
        self.transform_matrix = None

    def generate_perspective_transform_matrix(self, camera_keypoints, tactical_keypoints):
        camera_pts = np.array(camera_keypoints, dtype=np.float32)
        tactical_pts = np.array(tactical_keypoints, dtype=np.float32)
        self.transform_matrix, _ = cv2.findHomography(camera_pts, tactical_pts)
        print("[INFO] Perspective matrix generated.")

    def transform_points(self, points):
        """
        Transform a list of points from video frame to tactical map.
        points: List of (x, y)
        return: List of (x, y) transformed
        """
        if self.transform_matrix is None:
            raise ValueError("Transformation matrix not computed.")
        pts = np.array(points, dtype=np.float32).reshape(-1, 1, 2)
        transformed = cv2.perspectiveTransform(pts, self.transform_matrix)
        return transformed.reshape(-1, 2)

    def get_player_positions(self, frames_player_positions, court_keypoints_per_frame):
        """
        frames_player_positions: list of list of (x, y) for each frame
        court_keypoints_per_frame: list of validated court keypoints for each frame
        return: transformed player positions per frame
        """
        all_transformed_positions = []
        for i, player_points in enumerate(frames_player_positions):
            self.generate_perspective_transform_matrix(
                camera_keypoints=court_keypoints_per_frame[i],
                tactical_keypoints=self.get_static_tactical_keypoints()
            )
            transformed = self.transform_points(player_points)
            all_transformed_positions.append(transformed)
        return all_transformed_positions

    def get_static_tactical_keypoints(self):
        """
        Tr·∫£ v·ªÅ danh s√°ch c√°c keypoint c·ªë ƒë·ªãnh tr√™n tactical map
        C·∫ßn ƒë·ªìng nh·∫•t th·ª© t·ª± v·ªõi court keypoints trong real view
        """
        # V√≠ d·ª•: keypoint theo tactical map chu·∫©n (gi√° tr·ªã pixel tr√™n ·∫£nh tactical)
        return [
            [50, 50], [890, 50], [50, 450], [890, 450],  # 4 g√≥c s√¢n
            # C√°c ƒëi·ªÉm kh√°c n·∫øu c√≥
        ]
```
### 2. Gi·∫£i th√≠ch logic:
| Ph·∫ßn                                    | M·ª•c ƒë√≠ch                                                    |
| --------------------------------------- | ----------------------------------------------------------- |
| `generate_perspective_transform_matrix` | T·∫°o ma tr·∫≠n bi·∫øn ƒë·ªïi gi·ªØa camera view ‚Üî tactical map        |
| `transform_points`                      | √Åp d·ª•ng ph√©p bi·∫øn ƒë·ªïi n√†y ƒë·ªÉ chuy·ªÉn v·ªã tr√≠ c·∫ßu th·ªß sang map |
| `get_player_positions`                  | Th·ª±c hi·ªán to√†n b·ªô quy tr√¨nh n√†y qua t·ª´ng frame              |
| `get_static_tactical_keypoints`         | Keypoint c·ªë ƒë·ªãnh c·ªßa tactical map (chu·∫©n h√≥a layout)        |
### 3. V·∫Ω c·∫ßu th·ªß l√™n tactical view (v√≠ d·ª• trong TacticalViewDrawer):
```bash
class TacticalViewDrawer:
    def draw(self, frame, tactical_positions):
        for (x, y) in tactical_positions:
            if x == 0 and y == 0:
                continue  # skip invalid
            cv2.circle(frame, (int(x), int(y)), radius=5, color=(0, 255, 0), thickness=-1)
        return frame
```
### 4. T√≠ch h·ª£p v√†o main.py
```bash
# Gi·∫£ s·ª≠ ƒë√£ c√≥:
# - validated_keypoints: validated court keypoints per frame
# - player_positions: list of player (x, y) per frame

tactical_converter = TacticalViewConverter()
tactical_positions_per_frame = tactical_converter.get_player_positions(
    frames_player_positions=player_positions,
    court_keypoints_per_frame=validated_keypoints
)

# Trong loop v·∫Ω t·ª´ng frame:
for i, frame in enumerate(frames):
    tactical_frame = tactical_base_image.copy()
    tactical_frame = tactical_drawer.draw(tactical_frame, tactical_positions_per_frame[i])
    frame[0:500, 0:940] = tactical_frame  # overlay l√™n g√≥c frame g·ªëc
```
### K·∫øt qu·∫£:
- B·∫°n s·∫Ω c√≥ b·∫£n ƒë·ªì c·∫ßu th·ªß di chuy·ªÉn real-time tr√™n tactical map üî•.
- Chu·∫©n h√≥a d·ªØ li·ªáu ‚Üí l√†m ph√¢n t√≠ch chi·∫øn thu·∫≠t, heatmap, AI model ph√¢n t√≠ch chi·∫øn thu·∫≠t d·ªÖ d√†ng.

## 18. T√≠nh To√°n T·ªëc ƒê·ªô v√† Qu√£ng ƒê∆∞·ªùng
### M·ª•c ti√™u:
- T√≠nh qu√£ng ƒë∆∞·ªùng m·ªói c·∫ßu th·ªß di chuy·ªÉn gi·ªØa c√°c frame (tactical map).
- T√≠nh t·ªëc ƒë·ªô = qu√£ng ƒë∆∞·ªùng / th·ªùi gian.
- L∆∞u k·∫øt qu·∫£ v√† hi·ªÉn th·ªã tr·ª±c ti·∫øp trong video (tactical view).
### C·∫•u tr√∫c th∆∞ m·ª•c
```bash
speed_and_distance_calculator/
‚îÇ
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ speed_and_distance_calculator.py
```
### 1. `speed_and_distance_calculator.py`
```bash
import numpy as np
from utils.bbox_utils import measure_distance  # ho·∫∑c utils.geometry
import copy

class SpeedAndDistanceCalculator:
    def __init__(self, px_per_meter: float, frame_rate: float):
        self.px_per_meter = px_per_meter
        self.frame_rate = frame_rate
        self.prev_positions = {}  # player_id: (x, y)
        self.distances = {}       # frame_index: {player_id: distance}
        self.speeds = {}          # frame_index: {player_id: speed}

    def calculate_distance(self, tactical_player_positions: list, player_ids: list, frame_idx: int):
        """
        tactical_player_positions: [(x, y), ...] for this frame
        player_ids: matching IDs for each player in the same order
        frame_idx: index of current frame
        """
        frame_distances = {}
        frame_speeds = {}

        for pid, pos in zip(player_ids, tactical_player_positions):
            if pid not in self.prev_positions:
                self.prev_positions[pid] = pos
                continue

            prev_pos = self.prev_positions[pid]
            if pos == (0, 0) or prev_pos == (0, 0):  # Skip invalid
                continue

            dist_px = measure_distance(prev_pos, pos)
            dist_m = dist_px / self.px_per_meter
            speed_mps = dist_m * self.frame_rate

            frame_distances[pid] = dist_m
            frame_speeds[pid] = speed_mps

            self.prev_positions[pid] = pos  # update

        self.distances[frame_idx] = frame_distances
        self.speeds[frame_idx] = frame_speeds

        return frame_distances, frame_speeds
```
- Gi·∫£i th√≠ch logic: 

| Th√†nh ph·∫ßn                        | Gi·∫£i th√≠ch                                             |
| --------------------------------- | ------------------------------------------------------ |
| `prev_positions`                  | L∆∞u v·ªã tr√≠ c·∫ßu th·ªß ·ªü frame tr∆∞·ªõc ƒë·ªÉ so s√°nh            |
| `measure_distance(p1, p2)`        | Tr·∫£ v·ªÅ kho·∫£ng c√°ch Euclid gi·ªØa 2 ƒëi·ªÉm (tactical pixel) |
| `dist_m = px / px_per_meter`      | Chuy·ªÉn t·ª´ pixel sang m√©t                               |
| `speed = dist / (1 / frame_rate)` | M·ªói frame c√°ch nhau 1/30s ho·∫∑c 1/25s t√πy video         |

### 2. T√≠ch h·ª£p v·ªõi pipeline
Trong `main.py` (sau khi c√≥ tactical_positions v√† player_ids):
```bash
speed_calculator = SpeedAndDistanceCalculator(px_per_meter=50, frame_rate=30)  # v√≠ d·ª•

for i, frame in enumerate(frames):
    tactical_positions = tactical_positions_per_frame[i]
    player_ids = tracked_player_ids_per_frame[i]  # ph·∫£i tr√≠ch t·ª´ tracking

    dist, speed = speed_calculator.calculate_distance(tactical_positions, player_ids, i)
```
### 3. Hi·ªÉn th·ªã speed l√™n tactical map
Trong `TacticalViewDrawer`:
```bash
def draw_speed(self, frame, player_positions, player_ids, player_speeds):
    for (x, y), pid in zip(player_positions, player_ids):
        if pid not in player_speeds or (x, y) == (0, 0): continue
        speed = player_speeds[pid]
        label = f"{speed:.1f} m/s"
        cv2.putText(frame, label, (int(x) + 10, int(y) - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)
```

```bash
tactical_drawer.draw_speed(tactical_frame, tactical_positions, player_ids, speed_calculator.speeds[i])
```
### Output:
- M·ªói c·∫ßu th·ªß gi·ªù c√≥ d√≤ng hi·ªÉn th·ªã "3.5 m/s" ·ªü tactical map.
- C√≥ th·ªÉ export to√†n b·ªô distance/speed th√†nh CSV cho ph√¢n t√≠ch chi·∫øn thu·∫≠t/ML.
## 19. Th√™m Argument Parser v√† Config
M·ª•c ti√™u: Bi·∫øn main.py th√†nh m·ªôt module linh ho·∫°t, d·ªÖ test, d·ªÖ deploy, ch·∫°y nhanh v·ªõi c√°c video/option kh√°c nhau.
### 1. S·ª≠ d·ª•ng argparse trong main.py
```bash
import argparse

def define_parse_args():
    parser = argparse.ArgumentParser(description="Basketball Video Analysis Pipeline")

    parser.add_argument("--input_video", type=str, required=True,
                        help="Path to the input video")
    parser.add_argument("--output_video", type=str, default="outputs/annotated_output.mp4",
                        help="Path to save the output video")
    parser.add_argument("--stub_dir", type=str, default="stubs/",
                        help="Directory to save/load stub data (intermediate results)")

    parser.add_argument("--overwrite_stubs", action="store_true",
                        help="Force re-running all processing even if stubs exist")

    return parser
```
#### 2. S·ª≠ d·ª•ng trong main():
```bash
def main():
    parser = define_parse_args()
    args = parser.parse_args()

    input_path = args.input_video
    output_path = args.output_video
    stub_path = args.stub_dir
    overwrite = args.overwrite_stubs

    # Example usage
    video_frames = read_video(input_path)
    save_video(output_path, video_frames)
```
### 3. T·∫°o configs/ th∆∞ m·ª•c
```bash
configs/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ configs.py
```
- `configs.py`:
```bash
import os

# Default model paths
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

YOLO_DETECTION_MODEL = os.path.join(BASE_DIR, "../models/yolov8n.pt")
YOLO_POSE_MODEL = os.path.join(BASE_DIR, "../models/yolov8x-pose.pt")
DEFAULT_STUB_DIR = os.path.join(BASE_DIR, "../stubs/")
TACTICAL_COURT_IMAGE = os.path.join(BASE_DIR, "../assets/tactical_court.png")

# Other constants
FRAME_RATE = 30
PX_PER_METER = 50
```
- Trong `main.py`: 
```bash
from configs.configs import YOLO_DETECTION_MODEL, YOLO_POSE_MODEL, DEFAULT_STUB_DIR
```
## 20. Final Code Assembly & Testing
### Chu·ªói pipeline t·ªïng th·ªÉ trong main.py:
```bash
def main():
  # === 1. Parse Arguments & Configs ===
  parser = define_parse_args()
  args = parser.parse_args()

  input_path = args.input_video
  output_path = args.output_video
  stub_dir = args.stub_dir
  overwrite = args.overwrite_stubs

  # === 2. Read Video ===
  frames = read_video(input_path)  # list of BGR frames

  # === 3. Detect & Track Players ===
  player_detector = PlayerDetector(...)
  player_tracks = player_detector.detect_players(frames, stub_dir, overwrite)

  # === 4. Detect & Track Ball ===
  ball_tracker = BallTracker(...)
  ball_positions = ball_tracker.track_ball(frames, stub_dir, overwrite)

  # === 5. Assign Teams ===
  team_assigner = TeamAssigner(...)
  team_data = team_assigner.assign_teams(player_tracks, stub_dir, overwrite)

  # === 6. Calculate Ball Acquisition ===
  ball_acquisition = BallAcquisitionDetector(...)
  ball_possession = ball_acquisition.detect_ball_possession(
      frames, player_tracks, ball_positions, stub_dir, overwrite
  )

  # === 7. Team Ball Control (Statistical) ===
  team_ball_control = ball_acquisition.get_team_ball_control(team_data, ball_possession)

  # === 8. Detect Court Keypoints ===
  court_detector = CourtKeypointDetector(...)
  court_keypoints = court_detector.get_court_keypoints(frames, stub_dir, overwrite)

  # === 9. Validate Court Keypoints ===
  court_transformer = TacticalViewConverter(...)
  valid_keypoints = court_transformer.validate_keypoints(court_keypoints)

  # === 10. Transform Player Positions to Tactical View ===
  tactical_player_positions = court_transformer.get_player_positions(
      player_tracks, valid_keypoints, stub_dir, overwrite
  )

  # === 11. Calculate Speed & Distance ===
  speed_distance_calc = SpeedAndDistanceCalculator(...)
  player_speeds = speed_distance_calc.calculate_distance(tactical_player_positions, stub_dir, overwrite)

  # === 12. Draw All Annotations ===
  drawer = MainDrawer(...)  # Combines all drawing tools
  annotated_frames = drawer.draw_all(
      frames=frames,
      player_tracks=player_tracks,
      ball_positions=ball_positions,
      team_data=team_data,
      ball_possession=ball_possession,
      team_ball_control=team_ball_control,
      court_keypoints=valid_keypoints,
      tactical_player_positions=tactical_player_positions,
      player_speeds=player_speeds,
      court_overlay_path=TACTICAL_COURT_IMAGE
  )

  # === 13. Save Final Video ===
  save_video(output_path, annotated_frames)
```
### Nh·ªØng ƒëi·ªÉm quan tr·ªçng c·∫ßn test k·ªπ:
```bash
| Th√†nh ph·∫ßn                         | Test g√¨?                                           | Stub? |
| ---------------------------------- | -------------------------------------------------- | ----- |
| `read_video()`                     | ƒê·ªçc ƒë·∫ßy ƒë·ªß, kh√¥ng m·∫•t khung h√¨nh                   | ‚ùå     |
| `player_detector.detect_players()` | Bounding box + ID ·ªïn ƒë·ªãnh                          | ‚úÖ     |
| `ball_tracker.track_ball()`        | Ball center ch√≠nh x√°c                              | ‚úÖ     |
| `assign_teams()`                   | M·ªói player ƒë√∫ng team                               | ‚úÖ     |
| `detect_ball_possession()`         | Kh√¥ng b·ªã nh·∫•p nh√°y, ƒë√∫ng logic kho·∫£ng c√°ch         | ‚úÖ     |
| `court_keypoints`                  | C√°c ƒëi·ªÉm h·ª£p l√Ω, ƒë√∫ng v·ªã tr√≠ s√¢n                   | ‚úÖ     |
| `validate_keypoints()`             | Lo·∫°i b·ªè keypoint sai l·ªách r√µ r√†ng                  | ‚úÖ     |
| `transform_points()`               | V·ªã tr√≠ chuy·ªÉn sang tactical map kh√¥ng l·ªách nhi·ªÅu   | ‚úÖ     |
| `calculate_speed()`                | Player kh√¥ng d·ªãch chuy·ªÉn ‚Üí speed g·∫ßn 0             | ‚úÖ     |
| `draw_all()`                       | Overlay ƒë·∫ßy ƒë·ªß: track, ball, team, tactical, stats | ‚ùå     |
```